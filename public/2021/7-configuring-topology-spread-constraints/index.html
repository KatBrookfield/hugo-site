<!DOCTYPE html>
<html lang="en-us">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Hi, I&#39;m Hugo running on multiple Kubernetes nodes!  | 7 Configuring Topology Spread Constraints</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.7.1/css/bulma.min.css" />
    <link rel="stylesheet" href="http://advecti.io/css/blog.css" />
    </head>
<body>

    
    <nav class="navbar is-fixed-top" role="navigation" aria-label="main navigation">
        <div class="navbar-brand">
            <a class="navbar-item" href="http://advecti.io">Home</a>
        </div>
    </nav>
    

    
    <section class="hero is-info is-medium">
        <div class="hero-body" style="background-image: url(http://advecti.io/img/bg-blog.jpg);">
            <div class="container has-text-centered">
                <br>
                <h1 class="title is-size-1">
                    
                        7 Configuring Topology Spread Constraints
                    
                </h1>
                
            </div>
        </div>
    </section>


<div class="container">
    <div class="section">
    

<div class="columns">
    <div class="column is-9">
        <div class="tile is-child box">
            <div class="content">
                <p>In previous post, we have scaled our deployment by defining number of replicas required but we weren&rsquo;t able to control where they get deployed. Today, we&rsquo;ll look into this further.</p>
<h2 id="understanding-default-restart-behaviour">Understanding default restart behaviour</h2>
<p>It is important to note that if a node fails, pods running on that node are scheduled for deletion. Pods only get scheduled on a node once, when they are created, and remain running on that node until it stops or is terminated. That means that a pod is never rescheduled on another node, but a new pod is created instead.</p>
<p>In order to demonstrate this behaviour I have changed the number of replicas in my deployment to 2 and applied the changes:</p>
<pre><code>katarinabrookfield@KatsMac hugo-site % vi deployment.yml
katarinabrookfield@KatsMac hugo-site % kubectl apply -f deployment.yml
deployment.apps/hugo-site-deployment configured
katarinabrookfield@KatsMac hugo-site % kubectl get pod -o wide | grep hugo
hugo-site-deployment-74c977df86-wmc2x   1/1     Running            0          2m44s   10.2.1.86   lke27049-39949-60a53133d7b2   &lt;none&gt;           &lt;none&gt;
hugo-site-deployment-74c977df86-xp98x   1/1     Running            0          50m     10.2.1.85   lke27049-39949-60a53133d7b2   &lt;none&gt;           &lt;none&gt;
</code></pre><p>As you can see, both replicas are running on the same node. I then went into my Linode management console and powered off the node:</p>
<pre><code>katarinabrookfield@KatsMac hugo-site % kubectl get node -o wide
NAME                          STATUS     ROLES    AGE   VERSION   INTERNAL-IP       EXTERNAL-IP       OS-IMAGE                       KERNEL-VERSION         CONTAINER-RUNTIME
lke27049-39949-60a531334464   Ready      &lt;none&gt;   2d    v1.20.6   192.168.195.122   139.162.192.119   Debian GNU/Linux 9 (stretch)   5.10.0-6-cloud-amd64   docker://19.3.15
lke27049-39949-60a53133d7b2   NotReady   &lt;none&gt;   27d   v1.20.6   192.168.142.32    176.58.114.112    Debian GNU/Linux 9 (stretch)   5.10.0-6-cloud-amd64   docker://19.3.15
lke27049-39949-60a53134727c   Ready      &lt;none&gt;   27d   v1.20.6   192.168.198.132   178.79.152.181    Debian GNU/Linux 9 (stretch)   5.10.0-6-cloud-amd64   docker://19.3.15
</code></pre><p>By using the <em>describe</em> command I could see that the status of my containers has changed to <em>Ready:False</em>:</p>
<pre><code>katarinabrookfield@KatsMac hugo-site % kubectl describe pod hugo-site
Name:         hugo-site-deployment-74c977df86-wmc2x
.
.
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   True
  PodScheduled      True
  .
  .
</code></pre><p>Our website was no longer accessible; however, when looking at the pods, they still appeared to be running:</p>
<pre><code>katarinabrookfield@KatsMac hugo-site % kubectl get pod -o wide | grep hugo
hugo-site-deployment-74c977df86-wmc2x   1/1     Running            0          7m13s   10.2.1.86   lke27049-39949-60a53133d7b2   &lt;none&gt;           &lt;none&gt;
hugo-site-deployment-74c977df86-xp98x   1/1     Running            0          55m     10.2.1.85   lke27049-39949-60a53133d7b2   &lt;none&gt;           &lt;none&gt;
</code></pre><p>I have then checked my deployment and there I could see that my number of <em>replicas</em> changed to 2 unavailable. As that was 100%, it has violated the default policy which tolerates 25% unavailability. Our condition has also changed to <em>Available:False</em> because the minimum number was no longer available:</p>
<pre><code>katarinabrookfield@KatsMac hugo-site % kubectl describe deployment hugo-site
Name:                   hugo-site-deployment
Namespace:              default
CreationTimestamp:      Tue, 15 Jun 2021 19:01:37 +0100
Labels:                 &lt;none&gt;
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               project=hugo-site
Replicas:               2 desired | 2 updated | 2 total | 0 available | 2 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  project=hugo-site
  Containers:
   hugo-pod:
    Image:        katbrookfield/hugo-site
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      False   MinimumReplicasUnavailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   hugo-site-deployment-74c977df86 (2/2 replicas created)
</code></pre><p>Replicasets showed 0 Ready:</p>
<pre><code>katarinabrookfield@KatsMac hugo-site % kubectl get rs
NAME                              DESIRED   CURRENT   READY   AGE
hugo-site-deployment-74c977df86   2         2         0       65m
</code></pre><p>After few minutes, this has changed to 2 Ready:</p>
<pre><code>katarinabrookfield@KatsMac hugo-site % kubectl get rs
NAME                              DESIRED   CURRENT   READY   AGE
hugo-site-deployment-74c977df86   2         2         2       68m
</code></pre><p>Checking the pods again, we could see the old ones were marked as <em>Terminating</em> and new ones were deployed and <em>Running</em>:</p>
<pre><code>katarinabrookfield@KatsMac hugo-site % kubectl get pod -o wide | grep hugo
hugo-site-deployment-74c977df86-2sh5n   1/1     Running            0          2m5s   10.2.4.6    lke27049-39949-60a531334464   &lt;none&gt;           &lt;none&gt;
hugo-site-deployment-74c977df86-69qtt   1/1     Running            0          2m5s   10.2.4.3    lke27049-39949-60a531334464   &lt;none&gt;           &lt;none&gt;
hugo-site-deployment-74c977df86-wmc2x   1/1     Terminating        0          12m    10.2.1.86   lke27049-39949-60a53133d7b2   &lt;none&gt;           &lt;none&gt;
hugo-site-deployment-74c977df86-xp98x   1/1     Terminating        0          60m    10.2.1.85   lke27049-39949-60a53133d7b2   &lt;none&gt;           &lt;none&gt;
</code></pre><p>Our deployment now showed total number of available replicas as 2 and the condition has changed to <em>Available:True</em> as the minimum number of replicas was available again:</p>
<pre><code>katarinabrookfield@KatsMac hugo-site % kubectl describe deployment hugo-site
Name:                   hugo-site-deployment
Namespace:              default
CreationTimestamp:      Tue, 15 Jun 2021 19:01:37 +0100
Labels:                 &lt;none&gt;
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               project=hugo-site
Replicas:               2 desired | 2 updated | 2 total | 2 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  project=hugo-site
  Containers:
   hugo-pod:
    Image:        katbrookfield/hugo-site
    Port:         8080/TCP
    Host Port:    0/TCP
    Environment:  &lt;none&gt;
    Mounts:       &lt;none&gt;
  Volumes:        &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      True    MinimumReplicasAvailable
</code></pre><p>Once I started the old host back up, the old pods were deleted:</p>
<pre><code>katarinabrookfield@KatsMac hugo-site % kubectl get pod -o wide | grep hugo
hugo-site-deployment-74c977df86-2sh5n   1/1     Running            0          8m17s   10.2.4.6   lke27049-39949-60a531334464   &lt;none&gt;           &lt;none&gt;
hugo-site-deployment-74c977df86-69qtt   1/1     Running            0          8m17s   10.2.4.3   lke27049-39949-60a531334464   &lt;none&gt;           &lt;none&gt;
</code></pre><p>As soon as our pods have been successfully replaced our website became available again.
However, they are again running on the same node.</p>
<h2 id="configuring-topology-spread-constraints">Configuring Topology Spread Constraints</h2>
<p>In order to introduce some level of high-availability we are going to define rules to dictate how should our pods be spread across the cluster nodes.</p>
<h3 id="configuring-node-labels">Configuring Node labels</h3>
<p>One of the prerequisites is defining Node labels that will define our <em>topology domains</em> (a fault-domain basically). These could be based on regions, zones, nodes, or other user-defined settings. In our scenario, I will define a topology based on Nodes.</p>
<pre><code>katarinabrookfield@KatsMac hugo-site % kubectl label node lke27049-39949-60a531334464 node=node1
node/lke27049-39949-60a531334464 labeled
katarinabrookfield@KatsMac hugo-site % kubectl label node lke27049-39949-60a53133d7b2 node=node2
node/lke27049-39949-60a53133d7b2 labeled
</code></pre><p>You can verify that a label has been added either by running <em>kubectl get nodes &ndash;show-labels</em> or by using the <em>describe</em> command on a node. You will see the label added to existing labels:</p>
<pre><code>katarinabrookfield@KatsMac hugo-site % kubectl describe node lke27049-39949-60a531334464
Name:               lke27049-39949-60a531334464
Roles:              &lt;none&gt;
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/instance-type=g6-standard-1
                    beta.kubernetes.io/os=linux
                    failure-domain.beta.kubernetes.io/region=eu-west
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=lke27049-39949-60a531334464
                    kubernetes.io/os=linux
                    lke.linode.com/pool-id=39949
                    node=node1
</code></pre><h3 id="adding-topology-spread-constraints-to-a-deployment">Adding Topology Spread Constraints to a Deployment</h3>
<p>Before we proceed with adding the rules, we will quickly verify that the pods are running on one node only:</p>
<pre><code>katarinabrookfield@KatsMac hugo-site % kubectl get pod -o wide | grep hugo
hugo-site-deployment-6fb88df68-h78sp   1/1     Running            0          82s    10.2.4.8   lke27049-39949-60a531334464   &lt;none&gt;           &lt;none&gt;
hugo-site-deployment-6fb88df68-lj7z9   1/1     Running            0          86s    10.2.4.7   lke27049-39949-60a531334464   &lt;none&gt;           &lt;none&gt;
</code></pre><p>We are now going to modify our deployment to add the new rules. Edit the deployment.yml file and enter following data:</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: hugo-site-deployment
spec:
  replicas: 5
  selector:
    matchLabels:
      project: hugo-site
  template:
    metadata:
      labels:
        project: hugo-site
    spec: 
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: node
        whenUnsatisfiable: ScheduleAnyway
        labelSelector: 
          matchLabels:
            project: hugo-site
      containers:
      - name: hugo-pod
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
        image: katbrookfield/hugo-site
</code></pre><p>Note: As the <em>topologySpreadConstraints</em> is a <em>Pod</em> definition, we need to add it to Pod specs.</p>
<p>We have added following values:</p>
<ul>
<li>
<p><strong>maxSkew</strong> - describes the degree to which Pods may be unevenly distributed, in our case by 1</p>
</li>
<li>
<p><strong>topologyKey</strong> - is the label we have added</p>
</li>
<li>
<p><strong>whenUnsatisfiable</strong> - we have defined a soft rule that will try to satisfy the condition by minimizing the skew, the default option is <em>DoNotSchedule</em> which stops the pods from being scheduled</p>
</li>
<li>
<p><strong>labelSelector</strong> - matches our pod labels</p>
</li>
</ul>
<p>We will now deploy the changes, including an increase of replicas to 5:</p>
<pre><code>katarinabrookfield@KatsMac hugo-site % kubectl apply -f deployment.yml
deployment.apps/hugo-site-deployment configured
katarinabrookfield@KatsMac hugo-site % kubectl get pod -o wide | grep hugo
hugo-site-deployment-6fb88df68-6cbv5   0/1     ContainerCreating   0          2s     &lt;none&gt;     lke27049-39949-60a531334464   &lt;none&gt;           &lt;none&gt;
hugo-site-deployment-6fb88df68-h78sp   1/1     Running             0          111s   10.2.4.8   lke27049-39949-60a531334464   &lt;none&gt;           &lt;none&gt;
hugo-site-deployment-6fb88df68-jgtqv   0/1     ContainerCreating   0          2s     &lt;none&gt;     lke27049-39949-60a53133d7b2   &lt;none&gt;           &lt;none&gt;
hugo-site-deployment-6fb88df68-lj7z9   1/1     Running             0          115s   10.2.4.7   lke27049-39949-60a531334464   &lt;none&gt;           &lt;none&gt;
hugo-site-deployment-6fb88df68-ml7gz   0/1     ContainerCreating   0          2s     &lt;none&gt;     lke27049-39949-60a53133d7b2   &lt;none&gt;           &lt;none&gt;
katarinabrookfield@KatsMac hugo-site % kubectl get pod -o wide | grep hugo
hugo-site-deployment-6fb88df68-6cbv5   1/1     Running            0          13s    10.2.4.9   lke27049-39949-60a531334464   &lt;none&gt;           &lt;none&gt;
hugo-site-deployment-6fb88df68-h78sp   1/1     Running            0          2m2s   10.2.4.8   lke27049-39949-60a531334464   &lt;none&gt;           &lt;none&gt;
hugo-site-deployment-6fb88df68-jgtqv   1/1     Running            0          13s    10.2.1.6   lke27049-39949-60a53133d7b2   &lt;none&gt;           &lt;none&gt;
hugo-site-deployment-6fb88df68-lj7z9   1/1     Running            0          2m6s   10.2.4.7   lke27049-39949-60a531334464   &lt;none&gt;           &lt;none&gt;
hugo-site-deployment-6fb88df68-ml7gz   1/1     Running            0          13s    10.2.1.5   lke27049-39949-60a53133d7b2   &lt;none&gt;           &lt;none&gt;
</code></pre><p>3 new replicas have now been added and evenly distribuded across the two nodes we have labeled.</p>
<p>Our website will now remain available even when a node fails.</p>

            </div>
        </div>
    </div>
    <div class="column is-3">
        <div class="card">
    <div class="card-content">
        <h1 class="title is-5">Tags</h1>
        <div class="tags">
        
        </div>          
    </div>
</div><br>
        <div class="card">
    <div class="card-content">
        <h1 class="title is-5">Recent posts</h1>
        
            <h1><a href="http://advecti.io/2021/7-configuring-topology-spread-constraints/">7 Configuring Topology Spread Constraints</a></h1>
            <time class="has-text-grey-light is-size-7">15 June 2021</time>
        
            <h1><a href="http://advecti.io/2021/6-creating-a-deployment/">6 Creating a Deployment</a></h1>
            <time class="has-text-grey-light is-size-7">9 June 2021</time>
        
            <h1><a href="http://advecti.io/2021/5-creating-a-loadbalancer/">5 Creating a Loadbalancer</a></h1>
            <time class="has-text-grey-light is-size-7">7 June 2021</time>
        
            <h1><a href="http://advecti.io/2021/4-creating-a-pod/">4 Creating a Pod</a></h1>
            <time class="has-text-grey-light is-size-7">7 June 2021</time>
        
            <h1><a href="http://advecti.io/2021/3-creating-a-linode-kubernetes-cluster/">3 Creating a Linode Kubernetes Cluster</a></h1>
            <time class="has-text-grey-light is-size-7">7 June 2021</time>
        
    </div>
</div>
    <br>
                


    
<br>
        <div class="card">
    <div class="card-content">
        <h1 class="title is-5">Archives</h1>
        
            <a href="http://advecti.io/archives/2021">2021</a> (8)<br>
        
    </div>
</div>

    </div>
</div>


    </div>
</div>

<footer class="footer has-background-grey-darker has-text-white">
    <div class="content has-text-centered">
        <p>
            <span class="icon is-large"><a href="https://twitter.com/MrsBrookfield" class="mysocial" rel="me"><i class="fab fa-twitter fa-3x"></i></a></span>&nbsp;&nbsp;
            <br><br>
            Copyright &copy; Hi, I&#39;m Hugo running on multiple Kubernetes nodes! 2021 - Theme by <a href="https://jeffprod.com" class="mysocial">JeffProd.com</a>
            - <a class="mysocial" href="http://advecti.io/about">About</a>
        </p>
    </div>
</footer>

<script defer src="https://use.fontawesome.com/releases/v5.1.0/js/all.js"></script>
</body>
</html>
